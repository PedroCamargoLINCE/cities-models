{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4002118f",
   "metadata": {},
   "source": [
    "# Batch XGBoost Training for All Municipalities\n",
    "\n",
    "This notebook trains an XGBoost model for every municipality in the dataset using the best hyperparameters found for São Paulo. Results (predictions, metrics, model, and scaler) are saved in a subfolder for each municipality for reproducibility and easy deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f053fb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\pedro\\onedrive - unesp\\documentos\\github\\treinamento_clusters_hpc\\.conda\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4040fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path to the project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Add the project root to sys.path (not the src directory)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added {project_root} to sys.path\")\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from src.preprocessing import load_city_data, filter_city, clean_timeseries, prepare_data_for_model\n",
    "from src.train import evaluate_model, save_predictions, save_metrics\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# --- Set the best parameters found for São Paulo ---\n",
    "xgb_best_params = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'gamma': 0,\n",
    "    'min_child_weight': 1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# --- Data and output paths ---\n",
    "data_path = '../data/df_base_morb_resp.csv'\n",
    "results_dir = '../results/xgboost_batch_all_municipalities(morbresp)'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# --- Load all city codes ---\n",
    "df = load_city_data(data_path)\n",
    "city_codes = sorted(df['CD_MUN'].unique())\n",
    "\n",
    "# --- Model parameters ---\n",
    "model_params = {\n",
    "    'sequence_length': 8,\n",
    "    'forecast_horizon': 4,\n",
    "    'normalization': None,\n",
    "    'val_size': None\n",
    "}\n",
    "target_column = 'target'\n",
    "\n",
    "def reduce_to_1d(arr):\n",
    "    arr = np.asarray(arr)\n",
    "    if arr.ndim == 1:\n",
    "        return arr\n",
    "    if arr.ndim == 2:\n",
    "        if arr.shape[1] == 1:\n",
    "            return arr.ravel()\n",
    "        else:\n",
    "            return arr.sum(axis=1)\n",
    "    raise ValueError(f\"Unexpected array shape: {arr.shape}\")\n",
    "\n",
    "skipped_cities = []  # Collect skipped cities and reasons\n",
    "\n",
    "# --- Batch training ---\n",
    "for cd_mun in tqdm(city_codes, desc='Municipalities'):\n",
    "    city_name = str(cd_mun)\n",
    "    df_city = filter_city(df, cd_mun=cd_mun)\n",
    "    df_city = clean_timeseries(df_city, target_column=target_column)\n",
    "\n",
    "    # --- Handle NaNs in target column ---\n",
    "    target_nans = df_city[target_column].isna().sum()\n",
    "    total_rows = len(df_city)\n",
    "    nan_ratio = target_nans / total_rows if total_rows > 0 else 1\n",
    "    if nan_ratio > 0.3:\n",
    "        skipped_cities.append((city_name, f\"too many NaNs in target column ({nan_ratio:.1%})\"))\n",
    "        continue\n",
    "    if target_nans > 0:\n",
    "        df_city[target_column] = df_city[target_column].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    data_dict = prepare_data_for_model(\n",
    "        df=df_city,\n",
    "        target_column=target_column,\n",
    "        sequence_length=model_params['sequence_length'],\n",
    "        forecast_horizon=model_params['forecast_horizon'],\n",
    "        normalization=model_params['normalization'],\n",
    "        val_size=model_params.get('val_size', None)\n",
    "    )\n",
    "    X_train = data_dict['X_train']\n",
    "    y_train = data_dict['y_train']\n",
    "    X_test = data_dict['X_test']\n",
    "    y_test = data_dict['y_test']\n",
    "    test_df = data_dict['test_df']\n",
    "\n",
    "    # --- Data quality checks ---\n",
    "    skip_reason = None\n",
    "    if X_train.shape[0] < 2 or X_test.shape[0] < 1:\n",
    "        skip_reason = f\"Not enough samples (train: {X_train.shape[0]}, test: {X_test.shape[0]})\"\n",
    "    elif np.isnan(X_train).any() or np.isnan(X_test).any() or np.isnan(y_train).any() or np.isnan(y_test).any():\n",
    "        skip_reason = \"NaN values present in features or targets\"\n",
    "    elif np.all(X_train == 0) or np.all(X_test == 0):\n",
    "        skip_reason = \"All features are zero\"\n",
    "    elif np.all(y_train == 0) or np.all(y_test == 0):\n",
    "        skip_reason = \"All targets are zero\"\n",
    "    elif np.unique(y_train).size == 1:\n",
    "        skip_reason = f\"Target is constant: {y_train[0]}\"\n",
    "\n",
    "    if skip_reason:\n",
    "        skipped_cities.append((city_name, skip_reason))\n",
    "        continue\n",
    "\n",
    "    # Flatten for XGBoost\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    # --- Feature scaling ---\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    # Train and predict\n",
    "    xgb = XGBRegressor(**xgb_best_params)\n",
    "    xgb.fit(X_train_scaled, y_train)\n",
    "    y_pred = xgb.predict(X_test_scaled)\n",
    "    # Save model, scaler, predictions, and metrics in subfolder per city\n",
    "    city_dir = os.path.join(results_dir, city_name)\n",
    "    os.makedirs(city_dir, exist_ok=True)\n",
    "    model_file = os.path.join(city_dir, f'{city_name}_xgboost_model.json')\n",
    "    scaler_file = os.path.join(city_dir, f'{city_name}_scaler.pkl')\n",
    "    xgb.save_model(model_file)\n",
    "    joblib.dump(scaler, scaler_file)\n",
    "    # Save predictions and metrics\n",
    "    y_test_1d = reduce_to_1d(y_test)\n",
    "    y_pred_1d = reduce_to_1d(y_pred)\n",
    "    test_dates = test_df['week'].values[-len(y_test_1d):] if 'week' in test_df.columns else np.arange(len(y_test_1d))\n",
    "    preds_file = save_predictions(\n",
    "        y_true=y_test_1d,\n",
    "        y_pred=y_pred_1d,\n",
    "        dates=test_dates,\n",
    "        city_name=city_name,\n",
    "        model_name='xgboost',\n",
    "        output_dir=city_dir\n",
    "    )\n",
    "    metrics = evaluate_model(xgb, X_test_scaled, y_test)\n",
    "    metrics_file = save_metrics(\n",
    "        metrics=metrics,\n",
    "        city_name=city_name,\n",
    "        model_name='xgboost',\n",
    "        output_dir=city_dir,\n",
    "        params=model_params\n",
    "    )\n",
    "\n",
    "# --- Print skipped cities summary ---\n",
    "if skipped_cities:\n",
    "    print(\"\\nSummary of skipped cities:\")\n",
    "    for city, reason in skipped_cities:\n",
    "        print(f\"City {city} skipped: {reason}\")\n",
    "else:\n",
    "    print(\"\\nNo cities were skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e0be5",
   "metadata": {},
   "source": [
    "All predictions, metrics, models, and scalers are saved in a subfolder for each city under the results directory. This ensures full reproducibility and correct deployment for each municipality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
