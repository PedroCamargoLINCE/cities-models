{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e33a57",
   "metadata": {},
   "source": [
    "# Batch Deep Learning Training for All Brazilian State Capitals (Stacked GRU)\n",
    "\n",
    "This notebook trains the best deep learning model configuration (Stacked GRU, as determined in previous experiments) for each Brazilian state capital. Results are saved in a structured way for further analysis and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c84fe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rio Branco - X_train type: <class 'numpy.ndarray'>, shape: (1136, 12, 9)\n",
      "Rio Branco - y_train type: <class 'numpy.ndarray'>, shape: (1136,)\n",
      "Rio Branco - X_val type: <class 'numpy.ndarray'>, shape: (0,)\n",
      "Rio Branco - y_val type: <class 'numpy.ndarray'>, shape: (0,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{city_name} - X_train type: {type(X_train)}, shape: {getattr(X_train, 'shape', None)}\")\n",
    "print(f\"{city_name} - y_train type: {type(y_train)}, shape: {getattr(y_train, 'shape', None)}\")\n",
    "print(f\"{city_name} - X_val type: {type(X_val)}, shape: {getattr(X_val, 'shape', None)}\")\n",
    "print(f\"{city_name} - y_val type: {type(y_val)}, shape: {getattr(y_val, 'shape', None)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c49f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\cities-models\\cities-models\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Set results_dir to the root project results directory\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m results_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdl_batch_state_capitals_morb_resp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(results_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Batch GRU Profundo (Stacked) para Todas as Capitais Brasileiras\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path to the project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Add the project root to sys.path (not the src directory)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added {project_root} to sys.path\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.preprocessing import load_city_data, prepare_data_for_model, filter_city, clean_timeseries\n",
    "from src.models import build_stacked_gru\n",
    "from src.train import train_model, evaluate_model, generate_forecasts, save_predictions, save_metrics\n",
    "from src.utils import plot_forecast, plot_forecast_error, plot_training_history\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set results_dir to the root project results directory (notebooks/../results/...)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "results_dir = os.path.join(project_root, 'results', 'dl_batch_state_capitals_morb_resp')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "# Lista de capitais e seus códigos IBGE\n",
    "capitals = [\n",
    "    (\"Rio Branco\", 1200401), (\"Maceió\", 2704302), (\"Macapá\", 1600303), (\"Manaus\", 1302603),\n",
    "    (\"Salvador\", 2927408), (\"Fortaleza\", 2304400), (\"Brasília\", 5300108), (\"Vitória\", 3205309),\n",
    "    (\"Goiânia\", 5208707), (\"São Luís\", 2111300), (\"Cuiabá\", 5103403), (\"Campo Grande\", 5002704),\n",
    "    (\"Belo Horizonte\", 3106200), (\"Belém\", 1501402), (\"João Pessoa\", 2507507), (\"Curitiba\", 4106902),\n",
    "    (\"Recife\", 2611606), (\"Teresina\", 2211001), (\"Rio de Janeiro\", 3304557), (\"Natal\", 2408102),\n",
    "    (\"Porto Alegre\", 4314902), (\"Porto Velho\", 1100205), (\"Boa Vista\", 1400100), (\"Florianópolis\", 4205407),\n",
    "    (\"São Paulo\", 3550308), (\"Aracaju\", 2800308), (\"Palmas\", 1721000)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Parâmetros do modelo e dados\n",
    "model_params = {\n",
    "    'sequence_length': 12,\n",
    "    'forecast_horizon': 4,\n",
    "    'normalization': 'zscore',\n",
    "    'val_size': None\n",
    "}\n",
    "target_column = 'target'\n",
    "data_path = '../data/df_base_morb_resp.csv'\n",
    "df = load_city_data(data_path)\n",
    "\n",
    "\n",
    "\n",
    "def reduce_to_1d(arr):\n",
    "    arr = np.asarray(arr)\n",
    "    if arr.ndim == 1:\n",
    "        return arr\n",
    "    if arr.ndim == 2:\n",
    "        if arr.shape[1] == 1:\n",
    "            return arr.ravel()\n",
    "        else:\n",
    "            return arr.sum(axis=1)\n",
    "    raise ValueError(f\"Unexpected array shape: {arr.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "for city_name, cd_mun in tqdm(capitals, desc='Capitais'):\n",
    "    print(f\"\\n{'='*40}\\n{city_name} ({cd_mun})\")\n",
    "    df_city = filter_city(df, cd_mun=cd_mun)\n",
    "    # --- Robust interpolation for missing target values ---\n",
    "    if df_city[target_column].isna().all():\n",
    "        print(f\"SKIP: {city_name} ({cd_mun}) - All target values are NaN. Skipping this city.\")\n",
    "        with open(os.path.join(results_dir, \"skipped_cities_due_to_nan.txt\"), \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{city_name} ({cd_mun}): All target values are NaN\\n\")\n",
    "        continue\n",
    "    n_missing_before = df_city[target_column].isna().sum()\n",
    "    if n_missing_before > 0:\n",
    "        df_city[target_column] = df_city[target_column].interpolate(method='linear', limit_direction='both')\n",
    "        # Fill any remaining NaNs at the edges\n",
    "        df_city[target_column] = df_city[target_column].fillna(method='ffill').fillna(method='bfill')\n",
    "        n_missing_after = df_city[target_column].isna().sum()\n",
    "        print(f\"{city_name}: Imputed {n_missing_before - n_missing_after} missing target values (linear+ffill+bfill). Remaining NaNs: {n_missing_after}\")\n",
    "    print(f\"Shape após limpeza: {df_city.shape}\")\n",
    "\n",
    "    # --- Pipeline idêntico ao notebook individual ---\n",
    "    data_dict = prepare_data_for_model(\n",
    "        df=df_city,\n",
    "        target_column=target_column,\n",
    "        sequence_length=model_params['sequence_length'],\n",
    "        forecast_horizon=model_params['forecast_horizon'],\n",
    "        normalization=model_params['normalization'],\n",
    "        val_size=model_params['val_size']\n",
    "    )\n",
    "\n",
    "    X_train = data_dict['X_train']\n",
    "    y_train = data_dict['y_train']\n",
    "    X_val = data_dict.get('X_val', None)\n",
    "    y_val = data_dict.get('y_val', None)\n",
    "    X_test = data_dict['X_test']\n",
    "    y_test = data_dict['y_test']\n",
    "    test_df = data_dict['test_df']\n",
    "    scaler = data_dict.get('scaler')\n",
    "    feature_columns = data_dict.get('feature_columns', None)\n",
    "\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"feature_columns: {feature_columns}\")\n",
    "\n",
    "    # --- NaN check and handling ---\n",
    "    def has_nan(*arrays):\n",
    "        return any(np.isnan(arr).any() for arr in arrays if arr is not None)\n",
    "\n",
    "    nan_found = has_nan(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    if nan_found:\n",
    "        print(f\"WARNING: NaNs detected in data splits for {city_name}. Skipping this city.\")\n",
    "        with open(os.path.join(results_dir, \"skipped_cities_due_to_nan.txt\"), \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{city_name} ({cd_mun}): NaNs in data splits\\n\")\n",
    "        continue\n",
    "\n",
    "    input_shape = X_train.shape[1:]\n",
    "    model = build_stacked_gru(input_shape=input_shape, loss='huber')\n",
    "\n",
    "    history = train_model(\n",
    "        model=model,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        batch_size=32,\n",
    "        epochs=400,\n",
    "        patience=40,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # --- Avaliação e denormalização ---\n",
    "    y_pred = generate_forecasts(model, X_test)\n",
    "    y_test_1d = reduce_to_1d(y_test)\n",
    "    y_pred_1d = reduce_to_1d(y_pred)\n",
    "    if scaler is not None and hasattr(scaler, 'scale_') and hasattr(scaler, 'mean_'):\n",
    "        if scaler.scale_.shape[0] > 1:\n",
    "            y_test_1d = y_test_1d * scaler.scale_[0] + scaler.mean_[0]\n",
    "            y_pred_1d = y_pred_1d * scaler.scale_[0] + scaler.mean_[0]\n",
    "        else:\n",
    "            y_test_1d = scaler.inverse_transform(y_test_1d.reshape(-1, 1)).flatten()\n",
    "            y_pred_1d = scaler.inverse_transform(y_pred_1d.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # --- Robust NaN check before evaluation ---\n",
    "    if np.isnan(y_test_1d).any() or np.isnan(y_pred_1d).any():\n",
    "        print(f\"[SKIP] {city_name}: NaNs found in predictions or targets. Skipping evaluation and saving.\")\n",
    "        with open(os.path.join(results_dir, 'skipped_cities_due_to_nan.txt'), 'a', encoding='utf-8') as f:\n",
    "            f.write(f\"{city_name} ({cd_mun}): NaNs in y_test_1d or y_pred_1d\\n\")\n",
    "        continue\n",
    "\n",
    "    test_dates = test_df['week'].values[-len(y_test_1d):] if 'week' in test_df.columns else np.arange(len(y_test_1d))\n",
    "\n",
    "    metrics = evaluate_model(\n",
    "        model=model,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        scaler=scaler\n",
    "    )\n",
    "\n",
    "    print(\"Métricas de Avaliação:\")\n",
    "    print(f\"MAE: {metrics['mae']:.4f}\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"R²: {metrics['r2']:.4f}\")\n",
    "\n",
    "    # Salvar previsões\n",
    "    preds_file = save_predictions(\n",
    "        y_true=y_test_1d,\n",
    "        y_pred=y_pred_1d,\n",
    "        dates=test_dates,\n",
    "        city_name=city_name,\n",
    "        model_name='gru_stacked',\n",
    "        output_dir=results_dir\n",
    "    )\n",
    "    print(f\"Previsões salvas em: {preds_file}\")\n",
    "\n",
    "    # Salvar métricas\n",
    "    metrics_file = save_metrics(\n",
    "        metrics=metrics,\n",
    "        city_name=city_name,\n",
    "        model_name='gru_stacked',\n",
    "        output_dir=results_dir,\n",
    "        params=model_params\n",
    "    )\n",
    "    print(f\"Métricas salvas em: {metrics_file}\")\n",
    "\n",
    "    # Salvar histórico de treinamento\n",
    "    if history is not None:\n",
    "        fig = plot_training_history(history)\n",
    "        fig.savefig(os.path.join(results_dir, f'{city_name}_gru_stacked_training_history.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68bcef",
   "metadata": {},
   "source": [
    "All predictions, metrics, and training history plots are saved in the results/dl_batch_state_capitals directory, one file per capital. You can rerun this notebook for other targets or model configs as needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
