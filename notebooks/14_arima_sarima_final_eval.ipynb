{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1243eb",
   "metadata": {},
   "source": [
    "# ARIMA & SARIMA: Final Performance Evaluation and Comparison\n",
    "\n",
    "This notebook trains and evaluates ARIMA and SARIMA models for forecasting the sum of the next 4 weeks (monthly forecast) of respiratory morbidity rates in Brazilian municipalities. Results are compared with previous deep learning and baseline models.\n",
    "\n",
    "- **Models:** ARIMA, SARIMA\n",
    "- **Target:** Sum of next 4 weeks (monthly forecast)\n",
    "- **Approach:** Same data splits and evaluation as other notebooks for fair comparison.\n",
    "- **All code is modular and uses the same data pipeline as other models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b90ac47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\cities-models\\cities-models\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path to the project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Add the project root to sys.path (not the src directory)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added {project_root} to sys.path\")\n",
    "    import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from src.preprocessing import load_city_data, filter_city, clean_timeseries, prepare_data_for_model\n",
    "from src.train import evaluate_model, save_predictions, save_metrics\n",
    "from src.utils import plot_forecast, plot_forecast_error\n",
    "\n",
    "results_dir = os.path.join('results', 'arima_sarima')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f4d5b5",
   "metadata": {},
   "source": [
    "## Carregamento e Seleção dos Dados\n",
    "\n",
    "Selecione a cidade para avaliação. O pipeline é idêntico aos outros notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "559325ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected city shape: (1200, 11)\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: Carregar dados de uma cidade (ajuste o caminho conforme necessário)\n",
    "data_path = '../data/df_base_morb_resp.csv'\n",
    "df = load_city_data(data_path)\n",
    "CD_MUN_SELECTED = 3550308  # São Paulo\n",
    "selected_city_name = \"São Paulo\" # Define city name for plots\n",
    "df_city = filter_city(df, cd_mun=CD_MUN_SELECTED)\n",
    "df_city = clean_timeseries(df_city, target_column='target')\n",
    "print(f\"Selected city shape: {df_city.shape}\")\n",
    "\n",
    "# Ensure 'week' is a PeriodIndex for statsmodels compatibility\n",
    "if not pd.api.types.is_period_dtype(df_city['week']):\n",
    "    df_city['week'] = pd.to_datetime(df_city['week'])\n",
    "    df_city['week'] = df_city['week'].dt.to_period('W')\n",
    "\n",
    "def make_rolling_targets(series, forecast_horizon):\n",
    "    return series.rolling(window=forecast_horizon, min_periods=forecast_horizon).sum().shift(-forecast_horizon+1)\n",
    "\n",
    "model_params = {\n",
    "    'forecast_horizon': 4,  # Soma das próximas 4 semanas\n",
    "    'test_size': 40,        # Últimas 40 semanas para teste\n",
    "}\n",
    "target_column = 'target'\n",
    "\n",
    "df_city = df_city.set_index('week')\n",
    "df_city['target_sum4w'] = make_rolling_targets(df_city[target_column], model_params['forecast_horizon'])\n",
    "df_city = df_city.dropna(subset=['target_sum4w'])\n",
    "\n",
    "test_size = model_params['test_size']\n",
    "train = df_city.iloc[:-test_size]\n",
    "test = df_city.iloc[-test_size:]\n",
    "train_series = train['target_sum4w']\n",
    "test_series = test['target_sum4w']\n",
    "test_weeks = test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d97acd",
   "metadata": {},
   "source": [
    "## Diagnóstico e Transformações Automáticas (Pré-processamento)\n",
    "\n",
    "Antes de treinar ARIMA/SARIMA, o notebook verifica e aplica automaticamente as transformações necessárias para garantir estacionariedade e variância estável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b12832f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnóstico inicial da série de treino:\n",
      "ADF Statistic: -4.4071, p-value: 0.0003\n",
      "KPSS Statistic: 0.0788, p-value: 0.1000\n",
      "Série apresenta alta assimetria ou curtose. Aplicando transformação logarítmica.\n",
      "\n",
      "Testando estacionariedade após transformação (log1p):\n",
      "ADF Statistic: -3.6462, p-value: 0.0049\n",
      "KPSS Statistic: 0.0689, p-value: 0.1000\n",
      "Série já estacionária após transformação.\n",
      "\n",
      "Resumo da transformação: log1p, número de diferenças aplicadas: 0\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox\n",
    "\n",
    "# Função para testar estacionariedade\n",
    "def test_stationarity(series, alpha=0.05, verbose=True):\n",
    "    adf_result = adfuller(series.dropna(), autolag='AIC')\n",
    "    kpss_result = kpss(series.dropna(), regression='ct', nlags='auto')\n",
    "    if verbose:\n",
    "        print(f\"ADF Statistic: {adf_result[0]:.4f}, p-value: {adf_result[1]:.4f}\")\n",
    "        print(f\"KPSS Statistic: {kpss_result[0]:.4f}, p-value: {kpss_result[1]:.4f}\")\n",
    "    stationary = (adf_result[1] < alpha) and (kpss_result[1] > alpha)\n",
    "    return stationary, adf_result, kpss_result\n",
    "\n",
    "# Função para estabilizar variância\n",
    "def try_boxcox(series):\n",
    "    shifted = False\n",
    "    min_val = series.min()\n",
    "    if min_val <= 0:\n",
    "        series = series + abs(min_val) + 1\n",
    "        shifted = True\n",
    "    y_box, lam = boxcox(series.dropna())\n",
    "    return y_box, lam, shifted, min_val\n",
    "\n",
    "# Diagnóstico e transformação\n",
    "y = train_series.copy()\n",
    "print(\"Diagnóstico inicial da série de treino:\")\n",
    "stationary, adf_res, kpss_res = test_stationarity(y)\n",
    "\n",
    "# 1. Variance stabilization\n",
    "if y.skew() > 1 or y.kurt() > 5:\n",
    "    print(\"Série apresenta alta assimetria ou curtose. Aplicando transformação logarítmica.\")\n",
    "    y_trans = np.log1p(y)\n",
    "    trans_type = 'log1p'\n",
    "else:\n",
    "    try:\n",
    "        y_box, lam, shifted, min_val = try_boxcox(y)\n",
    "        if abs(lam - 1) > 0.1:\n",
    "            print(f\"Aplicando transformação Box-Cox (lambda={lam:.2f}).\")\n",
    "            y_trans = pd.Series(y_box, index=y.dropna().index)\n",
    "            trans_type = f'boxcox:{lam:.2f}'\n",
    "            if shifted:\n",
    "                print(f\"A série foi deslocada por {abs(min_val)+1} para garantir positividade.\")\n",
    "        else:\n",
    "            y_trans = y\n",
    "            trans_type = 'none'\n",
    "    except Exception as e:\n",
    "        print(f\"Box-Cox falhou: {e}. Usando série original.\")\n",
    "        y_trans = y\n",
    "        trans_type = 'none'\n",
    "\n",
    "# 2. Test stationarity after variance stabilization\n",
    "print(f\"\\nTestando estacionariedade após transformação ({trans_type}):\")\n",
    "stationary, adf_res, kpss_res = test_stationarity(y_trans)\n",
    "\n",
    "# 3. Differencing if needed\n",
    "ndiffs = 0\n",
    "if not stationary:\n",
    "    print(\"Série ainda não estacionária. Aplicando diferenciação de 1ª ordem.\")\n",
    "    y_trans = y_trans.diff().dropna()\n",
    "    ndiffs += 1\n",
    "    stationary, adf_res, kpss_res = test_stationarity(y_trans)\n",
    "    if not stationary:\n",
    "        print(\"Ainda não estacionária após 1ª diferença. Aplicando 2ª ordem.\")\n",
    "        y_trans = y_trans.diff().dropna()\n",
    "        ndiffs += 1\n",
    "        stationary, adf_res, kpss_res = test_stationarity(y_trans)\n",
    "    else:\n",
    "        print(\"Série estacionária após 1ª diferença.\")\n",
    "else:\n",
    "    print(\"Série já estacionária após transformação.\")\n",
    "\n",
    "print(f\"\\nResumo da transformação: {trans_type}, número de diferenças aplicadas: {ndiffs}\")\n",
    "\n",
    "# Salvar para uso posterior\n",
    "train_transformed = y_trans\n",
    "transformation_info = {'type': trans_type, 'ndiffs': ndiffs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cab033",
   "metadata": {},
   "source": [
    "## Treinamento e Previsão: ARIMA\n",
    "\n",
    "Ajuste um modelo ARIMA simples (sem sazonalidade) e gere previsões para o período de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3651b596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ARIMA with order: (1, 0, 1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "plot_forecast() got an unexpected keyword argument 'y_true'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 41\u001b[0m\n\u001b[0;32m     32\u001b[0m arima_metrics_file \u001b[38;5;241m=\u001b[39m save_metrics(\n\u001b[0;32m     33\u001b[0m     metrics\u001b[38;5;241m=\u001b[39marima_metrics,\n\u001b[0;32m     34\u001b[0m     city_name\u001b[38;5;241m=\u001b[39mselected_city_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m: arima_order}\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Plot ARIMA forecast and errors\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[43mplot_forecast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_series\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marima_forecast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_weeks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mARIMA Forecast for \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mselected_city_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marima_forecast_plot.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m plot_forecast_error(\n\u001b[0;32m     49\u001b[0m     y_true\u001b[38;5;241m=\u001b[39mtest_series\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[0;32m     50\u001b[0m     y_pred\u001b[38;5;241m=\u001b[39marima_forecast,\n\u001b[0;32m     51\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mARIMA Forecast Errors for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_city_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     52\u001b[0m     output_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(results_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marima_error_plot.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMétricas ARIMA ajustado automaticamente:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: plot_forecast() got an unexpected keyword argument 'y_true'"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Use transformed train series for ARIMA\n",
    "arima_order = (1, ndiffs, 1)\n",
    "print(f\"Fitting ARIMA with order: {arima_order}\")\n",
    "arima_model = ARIMA(train_transformed, order=arima_order)\n",
    "arima_fit = arima_model.fit()\n",
    "arima_forecast_trans = arima_fit.forecast(steps=test_size)\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "# Invert transformations for ARIMA forecast\n",
    "arima_forecast = invert_transform(arima_forecast_trans, trans_type, lam, shifted, min_val)\n",
    "\n",
    "arima_metrics = {}\n",
    "arima_metrics['mae'] = np.mean(np.abs(test_series.values - arima_forecast))\n",
    "arima_metrics['rmse'] = np.sqrt(np.mean((test_series.values - arima_forecast)**2))\n",
    "arima_metrics['r2'] = 1 - np.sum((test_series.values - arima_forecast)**2) / np.sum((test_series.values - np.mean(test_series.values))**2)\n",
    "\n",
    "arima_preds_file = save_predictions(\n",
    "    y_true=test_series.values,\n",
    "    y_pred=arima_forecast,\n",
    "    dates=test_weeks,\n",
    "    city_name=selected_city_name,\n",
    "    model_name='arima',\n",
    "    output_dir=results_dir\n",
    ")\n",
    "arima_metrics_file = save_metrics(\n",
    "    metrics=arima_metrics,\n",
    "    city_name=selected_city_name,\n",
    "    model_name='arima',\n",
    "    output_dir=results_dir,\n",
    "    params={'order': arima_order}\n",
    ")\n",
    "\n",
    "# Plot ARIMA forecast and errors\n",
    "plot_forecast(\n",
    "    y_true=test_series.values,\n",
    "    y_pred=arima_forecast,\n",
    "    dates=test_weeks,\n",
    "    title=f\"ARIMA Forecast for {selected_city_name}\",\n",
    "    output_path=os.path.join(results_dir, 'arima_forecast_plot.png')\n",
    ")\n",
    "plot_forecast_error(\n",
    "    y_true=test_series.values,\n",
    "    y_pred=arima_forecast,\n",
    "    title=f\"ARIMA Forecast Errors for {selected_city_name}\",\n",
    "    output_path=os.path.join(results_dir, 'arima_error_plot.png')\n",
    ")\n",
    "\n",
    "print(f\"\\nMétricas ARIMA ajustado automaticamente:\")\n",
    "print(f\"MAE: {arima_metrics['mae']:.2f}\")\n",
    "print(f\"RMSE: {arima_metrics['rmse']:.2f}\")\n",
    "print(f\"R2: {arima_metrics['r2']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aea4b9",
   "metadata": {},
   "source": [
    "## Treinamento e Previsão: SARIMA com Transformações\n",
    "\n",
    "O modelo será ajustado considerando as transformações e diferenciações necessárias detectadas na célula anterior. O forecast será revertido para a escala original automaticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b60c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajustando SARIMA com ordem (1, 0, 1) e sazonalidade (1, 1, 1, 52)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m SARIMAX(train_transformed, order\u001b[38;5;241m=\u001b[39morder, seasonal_order\u001b[38;5;241m=\u001b[39mseasonal_order)\n\u001b[1;32m---> 11\u001b[0m fit \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m forecast_trans \u001b[38;5;241m=\u001b[39m fit\u001b[38;5;241m.\u001b[39mforecast(steps\u001b[38;5;241m=\u001b[39mtest_size)\n\u001b[0;32m     13\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\treinamento_clusters_hpc\\.conda\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:705\u001b[0m, in \u001b[0;36mMLEModel.fit\u001b[1;34m(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\u001b[0m\n\u001b[0;32m    703\u001b[0m         flags[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhessian_method\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m optim_hessian\n\u001b[0;32m    704\u001b[0m     fargs \u001b[38;5;241m=\u001b[39m (flags,)\n\u001b[1;32m--> 705\u001b[0m     mlefit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(start_params, method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    706\u001b[0m                          fargs\u001b[38;5;241m=\u001b[39mfargs,\n\u001b[0;32m    707\u001b[0m                          maxiter\u001b[38;5;241m=\u001b[39mmaxiter,\n\u001b[0;32m    708\u001b[0m                          full_output\u001b[38;5;241m=\u001b[39mfull_output,\n\u001b[0;32m    709\u001b[0m                          disp\u001b[38;5;241m=\u001b[39mdisp, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    710\u001b[0m                          skip_hessian\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;66;03m# Just return the fitted parameters if requested\u001b[39;00m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_params:\n",
      "File \u001b[1;32mc:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\treinamento_clusters_hpc\\.conda\\lib\\site-packages\\statsmodels\\base\\model.py:566\u001b[0m, in \u001b[0;36mLikelihoodModel.fit\u001b[1;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_t\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    565\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Optimizer()\n\u001b[1;32m--> 566\u001b[0m xopt, retvals, optim_settings \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mhessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mretall\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;66;03m# Restore cov_type, cov_kwds and use_t\u001b[39;00m\n\u001b[0;32m    576\u001b[0m optim_settings\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "File \u001b[1;32mc:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\treinamento_clusters_hpc\\.conda\\lib\\site-packages\\statsmodels\\base\\optimizer.py:243\u001b[0m, in \u001b[0;36mOptimizer._fit\u001b[1;34m(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall)\u001b[0m\n\u001b[0;32m    240\u001b[0m     fit_funcs\u001b[38;5;241m.\u001b[39mupdate(extra_fit_funcs)\n\u001b[0;32m    242\u001b[0m func \u001b[38;5;241m=\u001b[39m fit_funcs[method]\n\u001b[1;32m--> 243\u001b[0m xopt, retvals \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mretall\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mhess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhessian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m optim_settings \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: method, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_params\u001b[39m\u001b[38;5;124m'\u001b[39m: start_params,\n\u001b[0;32m    249\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: maxiter, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_output\u001b[39m\u001b[38;5;124m'\u001b[39m: full_output,\n\u001b[0;32m    250\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: disp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfargs\u001b[39m\u001b[38;5;124m'\u001b[39m: fargs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m: callback,\n\u001b[0;32m    251\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretall\u001b[39m\u001b[38;5;124m'\u001b[39m: retall, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_fit_funcs\u001b[39m\u001b[38;5;124m\"\u001b[39m: extra_fit_funcs}\n\u001b[0;32m    252\u001b[0m optim_settings\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\treinamento_clusters_hpc\\.conda\\lib\\site-packages\\statsmodels\\base\\optimizer.py:660\u001b[0m, in \u001b[0;36m_fit_lbfgs\u001b[1;34m(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m approx_grad:\n\u001b[0;32m    658\u001b[0m     func \u001b[38;5;241m=\u001b[39m f\n\u001b[1;32m--> 660\u001b[0m retvals \u001b[38;5;241m=\u001b[39m optimize\u001b[38;5;241m.\u001b[39mfmin_l_bfgs_b(func, start_params, maxiter\u001b[38;5;241m=\u001b[39mmaxiter,\n\u001b[0;32m    661\u001b[0m                                  callback\u001b[38;5;241m=\u001b[39mcallback, args\u001b[38;5;241m=\u001b[39mfargs,\n\u001b[0;32m    662\u001b[0m                                  bounds\u001b[38;5;241m=\u001b[39mbounds, disp\u001b[38;5;241m=\u001b[39mdisp,\n\u001b[0;32m    663\u001b[0m                                  \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kwargs)\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_output:\n\u001b[0;32m    666\u001b[0m     xopt, fopt, d \u001b[38;5;241m=\u001b[39m retvals\n",
      "File \u001b[1;32mc:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\treinamento_clusters_hpc\\.conda\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:277\u001b[0m, in \u001b[0;36mfmin_l_bfgs_b\u001b[1;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[0;32m    267\u001b[0m callback \u001b[38;5;241m=\u001b[39m _wrap_callback(callback)\n\u001b[0;32m    268\u001b[0m opts \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxcor\u001b[39m\u001b[38;5;124m'\u001b[39m: m,\n\u001b[0;32m    269\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mftol\u001b[39m\u001b[38;5;124m'\u001b[39m: factr \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39meps,\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgtol\u001b[39m\u001b[38;5;124m'\u001b[39m: pgtol,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m: callback,\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxls\u001b[39m\u001b[38;5;124m'\u001b[39m: maxls}\n\u001b[1;32m--> 277\u001b[0m res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args\u001b[38;5;241m=\u001b[39margs, jac\u001b[38;5;241m=\u001b[39mjac, bounds\u001b[38;5;241m=\u001b[39mbounds,\n\u001b[0;32m    278\u001b[0m                        \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopts)\n\u001b[0;32m    279\u001b[0m d \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjac\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    280\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    281\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuncalls\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnfev\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    282\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnit\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnit\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    283\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarnflag\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[0;32m    284\u001b[0m f \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\treinamento_clusters_hpc\\.conda\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:441\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    433\u001b[0m _lbfgsb\u001b[38;5;241m.\u001b[39msetulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr, pgtol, wa,\n\u001b[0;32m    434\u001b[0m                iwa, task, lsave, isave, dsave, maxls, ln_task)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 441\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    444\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\treinamento_clusters_hpc\\.conda\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:345\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x(x)\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[1;32m--> 345\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\treinamento_clusters_hpc\\.conda\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:307\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_orig_grad \u001b[38;5;129;01min\u001b[39;00m FD_METHODS:\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\treinamento_clusters_hpc\\.conda\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:48\u001b[0m, in \u001b[0;36m_wrapper_grad.<locals>.wrapped1\u001b[1;34m(x, f0)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped1\u001b[39m(x, f0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     47\u001b[0m     ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m approx_derivative(\n\u001b[0;32m     49\u001b[0m         fun, x, f0\u001b[38;5;241m=\u001b[39mf0, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfinite_diff_options\n\u001b[0;32m     50\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\treinamento_clusters_hpc\\.conda\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:523\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[1;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[0;32m    520\u001b[0m     use_one_sided \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\treinamento_clusters_hpc\\.conda\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:596\u001b[0m, in \u001b[0;36m_dense_difference\u001b[1;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[0;32m    594\u001b[0m     x1[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m h[i]\n\u001b[0;32m    595\u001b[0m     dx \u001b[38;5;241m=\u001b[39m x1[i] \u001b[38;5;241m-\u001b[39m x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[1;32m--> 596\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m f0\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-point\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[0;32m    598\u001b[0m     x1[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m h[i]\n",
      "File \u001b[1;32mc:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\treinamento_clusters_hpc\\.conda\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:474\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(x\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal floating\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    472\u001b[0m     x \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(x, x0\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m--> 474\u001b[0m f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(fun(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` return value has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore than 1 dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\treinamento_clusters_hpc\\.conda\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:21\u001b[0m, in \u001b[0;36m_wrapper_fun.<locals>.wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     17\u001b[0m ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\treinamento_clusters_hpc\\.conda\\lib\\site-packages\\statsmodels\\base\\model.py:534\u001b[0m, in \u001b[0;36mLikelihoodModel.fit.<locals>.f\u001b[1;34m(params, *args)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m(params, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m--> 534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m nobs\n",
      "File \u001b[1;32mc:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\treinamento_clusters_hpc\\.conda\\lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:940\u001b[0m, in \u001b[0;36mMLEModel.loglike\u001b[1;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m complex_step:\n\u001b[0;32m    938\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minversion_method\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m INVERT_UNIVARIATE \u001b[38;5;241m|\u001b[39m SOLVE_LU\n\u001b[1;32m--> 940\u001b[0m loglike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssm\u001b[38;5;241m.\u001b[39mloglike(complex_step\u001b[38;5;241m=\u001b[39mcomplex_step, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    942\u001b[0m \u001b[38;5;66;03m# Koopman, Shephard, and Doornik recommend maximizing the average\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;66;03m# likelihood to avoid scale issues, but the averaging is done\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;66;03m# automatically in the base model `fit` method\u001b[39;00m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loglike\n",
      "File \u001b[1;32mc:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\treinamento_clusters_hpc\\.conda\\lib\\site-packages\\statsmodels\\tsa\\statespace\\kalman_filter.py:1001\u001b[0m, in \u001b[0;36mKalmanFilter.loglike\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;124;03mCalculate the loglikelihood associated with the statespace model.\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;124;03m    The joint loglikelihood.\u001b[39;00m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    999\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconserve_memory\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1000\u001b[0m                   MEMORY_CONSERVE \u001b[38;5;241m^\u001b[39m MEMORY_NO_LIKELIHOOD)\n\u001b[1;32m-> 1001\u001b[0m kfilter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1002\u001b[0m loglikelihood_burn \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloglikelihood_burn\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1003\u001b[0m                                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloglikelihood_burn)\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconserve_memory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m&\u001b[39m MEMORY_NO_LIKELIHOOD):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "\n",
    "# Fixed SARIMA parameters\n",
    "order = (1, ndiffs, 1)\n",
    "seasonal_order = (1, 1, 1, 52)\n",
    "\n",
    "print(f\"Ajustando SARIMA com ordem {order} e sazonalidade {seasonal_order}...\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "model = SARIMAX(train_transformed, order=order, seasonal_order=seasonal_order)\n",
    "fit = model.fit(disp=False)\n",
    "forecast_trans = fit.forecast(steps=test_size)\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "# Invert transformations for SARIMA forecast\n",
    "forecast_final = invert_transform(forecast_trans, trans_type, lam, shifted, min_val)\n",
    "\n",
    "sarima_metrics = {}\n",
    "sarima_metrics['mae'] = np.mean(np.abs(test_series.values - forecast_final))\n",
    "sarima_metrics['rmse'] = np.sqrt(np.mean((test_series.values - forecast_final)**2))\n",
    "sarima_metrics['r2'] = 1 - np.sum((test_series.values - forecast_final)**2) / np.sum((test_series.values - np.mean(test_series.values))**2)\n",
    "\n",
    "sarima_preds_file = save_predictions(\n",
    "    y_true=test_series.values,\n",
    "    y_pred=forecast_final,\n",
    "    dates=test_weeks,\n",
    "    city_name=selected_city_name,\n",
    "    model_name='sarima',\n",
    "    output_dir=results_dir\n",
    ")\n",
    "sarima_metrics_file = save_metrics(\n",
    "    metrics=sarima_metrics,\n",
    "    city_name=selected_city_name,\n",
    "    model_name='sarima',\n",
    "    output_dir=results_dir,\n",
    "    params={'order': order, 'seasonal_order': seasonal_order}\n",
    ")\n",
    "\n",
    "# Plot SARIMA forecast and errors\n",
    "plot_forecast(\n",
    "    y_true=test_series.values,\n",
    "    y_pred=forecast_final,\n",
    "    dates=test_weeks,\n",
    "    title=f\"SARIMA Forecast for {selected_city_name}\",\n",
    "    output_path=os.path.join(results_dir, 'sarima_forecast_plot.png')\n",
    ")\n",
    "plot_forecast_error(\n",
    "    y_true=test_series.values,\n",
    "    y_pred=forecast_final,\n",
    "    title=f\"SARIMA Forecast Errors for {selected_city_name}\",\n",
    "    output_path=os.path.join(results_dir, 'sarima_error_plot.png')\n",
    ")\n",
    "\n",
    "print(f\"\\nMétricas SARIMA ajustado automaticamente:\")\n",
    "print(f\"MAE: {sarima_metrics['mae']:.2f}\")\n",
    "print(f\"RMSE: {sarima_metrics['rmse']:.2f}\")\n",
    "print(f\"R2: {sarima_metrics['r2']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
