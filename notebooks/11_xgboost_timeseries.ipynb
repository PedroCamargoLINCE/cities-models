{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80980a26",
   "metadata": {},
   "source": [
    "# XGBoost para Previsão Multivariada (Soma das Próximas 4 Semanas)\n",
    "Este notebook demonstra o treinamento e avaliação de um regressor XGBoost para prever a soma das próximas 4 semanas (previsão mensal) das taxas de morbidade respiratória em um município brasileiro selecionado, usando todas as features disponíveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2147904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\pedro\\OneDrive - Unesp\\Documentos\\GitHub\\cities-models\\cities-models\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path to the project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Add the project root to sys.path (not the src directory)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added {project_root} to sys.path\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from src.preprocessing import load_city_data, filter_city, clean_timeseries, prepare_data_for_model\n",
    "from src.utils import plot_forecast, plot_forecast_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e4f67",
   "metadata": {},
   "source": [
    "## Carregamento e Seleção da Cidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c9af15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected city shape: (1200, 11)\n"
     ]
    }
   ],
   "source": [
    "# Caminho dos dados\n",
    "csv_path = '../data/df_base_morb_resp.csv'\n",
    "df = load_city_data(csv_path)\n",
    "\n",
    "# Selecione a cidade (por código IBGE)\n",
    "CD_MUN_SELECTED = 3550308  # sp\n",
    "df_city = filter_city(df, cd_mun=CD_MUN_SELECTED)\n",
    "df_city = clean_timeseries(df_city, target_column='target')\n",
    "print(f\"Selected city shape: {df_city.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a854910",
   "metadata": {},
   "source": [
    "## Pré-processamento\n",
    "Prepare os dados para o XGBoost. Use todas as features exceto 'CD_MUN' e 'week'. O alvo é a soma das próximas 4 semanas ('target')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9be678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'sequence_length': 12,  # Use 12 semanas de histórico\n",
    "    'forecast_horizon': 4, # Soma das próximas 4 semanas\n",
    "    'normalization': None, # Sem normalização para XGBoost\n",
    "    'val_size': None\n",
    "}\n",
    "target_column = 'target'\n",
    "\n",
    "data_dict = prepare_data_for_model(\n",
    "    df=df_city,\n",
    "    target_column=target_column,\n",
    "    sequence_length=model_params['sequence_length'],\n",
    "    forecast_horizon=model_params['forecast_horizon'],\n",
    "    normalization=model_params['normalization'],\n",
    "    val_size=model_params.get('val_size', None)\n",
    ")\n",
    "\n",
    "X_train = data_dict['X_train']\n",
    "y_train = data_dict['y_train']\n",
    "X_val = data_dict.get('X_val', None)\n",
    "y_val = data_dict.get('y_val', None)\n",
    "X_test = data_dict['X_test']\n",
    "y_test = data_dict['y_test']\n",
    "test_df = data_dict['test_df']\n",
    "feature_columns = data_dict.get('feature_columns', None)\n",
    "\n",
    "# Flatten input for XGBoost\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "if X_val is not None:\n",
    "    X_val = X_val.reshape(X_val.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d1d75",
   "metadata": {},
   "source": [
    "## Observação sobre Normalização/Padronização\n",
    "\n",
    "Para modelos baseados em árvores como XGBoost, a padronização (z-score, StandardScaler) é geralmente preferida, pois é robusta a outliers e funciona bem para dados com diferentes escalas e sinais. MinMaxScaler pode ser útil se todos os dados forem positivos e você quiser preservar a faixa original, mas é mais sensível a outliers. Aqui, usamos StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Padronização (z-score) dos dados de entrada\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "if X_val is not None:\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "else:\n",
    "    X_val_scaled = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe67d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar scaler para uso futuro/deployment\n",
    "results_dir = os.path.join(project_root, 'results', 'xgboost_timeseries_individual')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "scaler_file = os.path.join(results_dir, f'{CD_MUN_SELECTED}_scaler.pkl')\n",
    "joblib.dump(scaler, scaler_file)\n",
    "print(f\"Scaler salvo em: {scaler_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5402c7ef",
   "metadata": {},
   "source": [
    "## Treinamento e Busca de Hiperparâmetros do Modelo XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f945fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [2, 3, 4, 5, 6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'min_child_weight': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "n_iter = 100\n",
    "param_list = list(ParameterSampler(param_dist, n_iter=n_iter, random_state=42))\n",
    "best_mae = float('inf')\n",
    "best_model = None\n",
    "best_params = None\n",
    "\n",
    "for i, params in enumerate(param_list):\n",
    "    xgb = XGBRegressor(random_state=42, **params)\n",
    "    xgb.fit(X_train_scaled, y_train)\n",
    "    if X_val_scaled is not None and y_val is not None:\n",
    "        y_val_pred = xgb.predict(X_val_scaled)\n",
    "        val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    else:\n",
    "        y_val_pred = xgb.predict(X_test_scaled)\n",
    "        val_mae = mean_absolute_error(y_test, y_val_pred)\n",
    "    if val_mae < best_mae:\n",
    "        best_mae = val_mae\n",
    "        best_model = xgb\n",
    "        best_params = params\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f\"Iteration {i+1}/{n_iter} - Current Best MAE: {best_mae:.4f}\")\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best validation MAE:\", best_mae)\n",
    "\n",
    "# Save the best model\n",
    "model_file = os.path.join(results_dir, f'{CD_MUN_SELECTED}_xgboost_model.json')\n",
    "best_model.save_model(model_file)\n",
    "print(f\"Modelo salvo em: {model_file}\")\n",
    "\n",
    "# Use the best model for test predictions\n",
    "y_pred = best_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e308bb",
   "metadata": {},
   "source": [
    "## Avaliação do Modelo (Métricas no Escopo Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7599bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_1d(arr):\n",
    "    arr = np.asarray(arr)\n",
    "    if arr.ndim == 1:\n",
    "        return arr\n",
    "    if arr.ndim == 2:\n",
    "        if arr.shape[1] == 1:\n",
    "            return arr.ravel()\n",
    "        else:\n",
    "            return arr.sum(axis=1)\n",
    "    raise ValueError(f\"Unexpected array shape: {arr.shape}\")\n",
    "\n",
    "# Garantir arrays 1D para métricas e visualização\n",
    "y_test_1d = reduce_to_1d(y_test)\n",
    "y_pred_1d = reduce_to_1d(y_pred)\n",
    "\n",
    "mae = mean_absolute_error(y_test_1d, y_pred_1d)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_1d, y_pred_1d))\n",
    "r2 = r2_score(y_test_1d, y_pred_1d)\n",
    "print(f\"MAE: {mae:.2f}\\nRMSE: {rmse:.2f}\\nR²: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243fb6ef",
   "metadata": {},
   "source": [
    "## Visualização: Previsão vs Real e Erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e27248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eixo temporal\n",
    "if 'week' in test_df.columns:\n",
    "    test_dates = test_df['week'].values[-len(y_test_1d):]\n",
    "else:\n",
    "    test_dates = np.arange(len(y_test_1d))\n",
    "\n",
    "# 1. Forecast vs Actual\n",
    "fig1 = plot_forecast(\n",
    "    true_values=y_test_1d,\n",
    "    predictions=y_pred_1d,\n",
    "    dates=test_dates,\n",
    "    title=\"XGBoost: Previsão vs Real\",\n",
    "    true_label=\"Real\",\n",
    "    pred_label=\"Previsto XGBoost\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# 2. Forecast Error\n",
    "fig2 = plot_forecast_error(\n",
    "    true_values=y_test_1d,\n",
    "    predictions=y_pred_1d,\n",
    "    dates=test_dates,\n",
    "    title=\"XGBoost: Erro de Previsão\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# 3. Distribution of Errors\n",
    "erros = y_pred_1d - y_test_1d\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(erros, kde=True, bins=30, color='crimson')\n",
    "plt.title('Distribution of Forecast Errors (XGBoost)')\n",
    "plt.xlabel('Error (Predicted - Actual)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 4. Scatter Plot: Actual vs Predicted\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(y_test_1d, y_pred_1d, alpha=0.5)\n",
    "plt.plot([y_test_1d.min(), y_test_1d.max()], [y_test_1d.min(), y_test_1d.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted Scatter (XGBoost)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 5. Residuals Over Time\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(test_dates, erros, marker='o', linestyle='-', color='orange')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.title('Residuals Over Time (XGBoost)')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Residual (Predicted - Actual)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 6. Rolling Mean of Errors\n",
    "window = 8\n",
    "rolling_error = pd.Series(erros).rolling(window=window).mean()\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(test_dates, rolling_error, color='purple')\n",
    "plt.title(f'Rolling Mean of Errors (window={window})')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Rolling Mean Error')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 7. Cumulative Error\n",
    "cumulative_error = np.cumsum(erros)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(test_dates, cumulative_error, color='teal')\n",
    "plt.title('Cumulative Forecast Error (XGBoost)')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Cumulative Error')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b2110",
   "metadata": {},
   "source": [
    "## Importância das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30392c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = best_model.feature_importances_\n",
    "if feature_columns is not None:\n",
    "    n_lags = X_train.shape[1] // len(feature_columns)\n",
    "    feature_names = [f\"{col}_t-{lag}\" for lag in range(model_params['sequence_length'], 0, -1) for col in feature_columns]\n",
    "else:\n",
    "    feature_names = [f\"f{i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "plt.figure(figsize=(12,6))\n",
    "feat_imp.head(20).plot(kind='bar')\n",
    "plt.title('Top 20 Feature Importances (XGBoost)')\n",
    "plt.ylabel('Importance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd122fe6",
   "metadata": {},
   "source": [
    "## Salvar Previsões e Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08371b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import save_predictions, save_metrics\n",
    "# Salvar previsões\n",
    "preds_file = save_predictions(\n",
    "    y_true=y_test_1d,\n",
    "    y_pred=y_pred_1d,\n",
    "    dates=test_dates,\n",
    "    city_name='Campinas',\n",
    "    model_name='xgboost',\n",
    "    output_dir=results_dir\n",
    ")\n",
    "print(f\"Previsões salvas em: {preds_file}\")\n",
    "\n",
    "# Salvar métricas\n",
    "metrics = {'mae': mae, 'rmse': rmse, 'r2': r2}\n",
    "metrics_file = save_metrics(\n",
    "    metrics=metrics,\n",
    "    city_name='Campinas',\n",
    "    model_name='xgboost',\n",
    "    output_dir=results_dir,\n",
    "    params=model_params\n",
    ")\n",
    "print(f\"Métricas salvas em: {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0275ad2e",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "O XGBoost multivariado permite prever a soma das próximas 4 semanas usando todas as features disponíveis. Compare seu desempenho com os modelos neurais e baselines nos outros notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
